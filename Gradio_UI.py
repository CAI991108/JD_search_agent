# Gradio_UI_test.py
import gradio as gr
import asyncio
import time, json, traceback
from typing import Dict, List, Tuple
from langgraph.graph import StateGraph

# å»¶è¿Ÿå¯¼å…¥AIMessageï¼Œé¿å…å¾ªç¯å¯¼å…¥
from langchain_core.messages.ai import AIMessage

# å£°æ˜JD_QueryStateç±»å‹ï¼Œé¿å…ç›´æ¥ä»appå¯¼å…¥
try:
    from app import JD_QueryState
except ImportError:
    from typing_extensions import TypedDict
    from typing import Annotated
    from langgraph.graph.message import add_messages

    class JD_QueryState(TypedDict):
        """State representing the user conversation of the JD product query."""
        messages: Annotated[list, add_messages]
        query: list[str]
        finished: bool

# ================== å¢å¼ºæ¶ˆæ¯å¤„ç†å™¨ ==================
class MessageProcessor:
    @staticmethod
    def parse(step_data: Dict) -> List[Tuple[str, str]]:
        """æ·±åº¦æ¶ˆæ¯è§£æå™¨ï¼ˆæ”¯æŒæ‰€æœ‰å·²çŸ¥ç»“æ„ï¼‰"""
        messages = []
        
        # å±‚çº§1ï¼šç›´æ¥æ¶ˆæ¯å­—æ®µ
        messages.extend(MessageProcessor._parse_level(step_data.get("messages", []))) 
        
        # å±‚çº§2ï¼šchatbotèŠ‚ç‚¹åµŒå¥—
        chatbot_data = step_data.get("chatbot", {})
        messages.extend(MessageProcessor._parse_level(chatbot_data.get("messages", [])))  
        messages.extend(MessageProcessor._parse_level(chatbot_data.get("output", [])))  
        
        # å±‚çº§3ï¼šå…¶ä»–å¯èƒ½å­—æ®µ
        for key in ["output", "response", "result"]:
            messages.extend(MessageProcessor._parse_level(step_data.get(key, [])))  
        
        # è¾“å‡ºæ¥æ”¶åˆ°çš„æ¶ˆæ¯æ•°
        msg_count = len(messages)
        assistant_count = sum(1 for role, _ in messages if role == "assistant")
        user_count = sum(1 for role, _ in messages if role == "user")
        print(f"ğŸ“¤ è§£æåˆ°{msg_count}æ¡æ¶ˆæ¯(ç”¨æˆ·:{user_count}, åŠ©æ‰‹:{assistant_count})")
        
        return messages
        
    @staticmethod
    def get_latest_assistant_message(messages: List[Tuple[str, str]]) -> str:
        """ä»è§£æçš„æ¶ˆæ¯åˆ—è¡¨ä¸­è·å–æœ€æ–°çš„åŠ©æ‰‹æ¶ˆæ¯"""
        # ç­›é€‰å‡ºæ‰€æœ‰åŠ©æ‰‹æ¶ˆæ¯
        assistant_messages = [content for role, content in messages if role == "assistant"]
        
        # è¿”å›æœ€åä¸€æ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        return assistant_messages[-1] if assistant_messages else ""

    @staticmethod
    def _parse_level(msg_list: List) -> List[Tuple[str, str]]:
        """ç»Ÿä¸€è§£æå±‚çº§æ•°æ®"""
        parsed = []
        for idx, item in enumerate(msg_list):
            role, content = "", ""
            log_prefix = ""
            
            # æ ¹æ®æ¶ˆæ¯ç±»å‹åˆ†åˆ«å¤„ç†
            message_type = type(item).__name__
            
            # AIMessageå¤„ç†
            if isinstance(item, AIMessage):
                role, content = "assistant", item.content
                log_prefix = f"ğŸ¤– [L1] AIMessage-{idx}"
            
            # å­—å…¸æ¶ˆæ¯å¤„ç†
            elif isinstance(item, dict):
                role = item.get("role", "unknown").lower()
                content = str(item.get("content", ""))
                
                # ç‰¹æ®Šå¤„ç†å·¥å…·æ¶ˆæ¯
                if role == "tool":
                    role = "assistant"
                    content = f"å·¥å…·æ‰§è¡Œç»“æœ: {content[:120]}..." if len(content) > 120 else content
                    log_prefix = f"âš™ï¸ [L1] å·¥å…·æ¶ˆæ¯-{idx}"
                elif role in ("user", "assistant"):
                    log_prefix = f"ğŸ“¨ [L1] å­—å…¸æ¶ˆæ¯-{idx}"
            
            # å…ƒç»„æ¶ˆæ¯å¤„ç†
            elif isinstance(item, tuple) and len(item) == 2:
                role, content = str(item[0]).lower(), str(item[1])
                log_prefix = f"ğŸ“¦ [L1] å…ƒç»„æ¶ˆæ¯-{idx}"
            
            # æ·»åŠ æœ‰æ•ˆçš„æ¶ˆæ¯åˆ°ç»“æœä¸­
            if role in ("user", "assistant") and content:
                parsed.append((role, content))
                print(f"{log_prefix}: {role}->{content[:50]}...")
        
        return parsed

# ================== æ™ºèƒ½æµå¼å¤„ç†å™¨ ==================
class StreamProcessor:
    def __init__(self, state_graph: StateGraph):
        self.state_graph = state_graph
        self.context = {
            "current_turn": 0,
            "history_hash": "",
            "response_cache": set(),
            "last_state": None
        }

    # +++ æ–°å¢æ–¹æ³• +++
    def reset_context(self):
        """é‡ç½®å¯¹è¯ä¸Šä¸‹æ–‡"""
        self.context = {
            "current_turn": 0,
            "history_hash": "",
            "response_cache": set(),
            "last_state": None
        }
        print("ğŸ”„ å·²é‡ç½®å¤„ç†å™¨ä¸Šä¸‹æ–‡")

    async def process(self, initial_state: JD_QueryState):
        """ç®€åŒ–çš„å¯¹è¯å¤„ç†å™¨ï¼Œæ·»åŠ äº†å·¥å…·è°ƒç”¨æ”¯æŒå’Œè¶…æ—¶æ§åˆ¶"""
        try:
            # è¾“å‡ºä»£ç†çŠ¶æ€
            import os
            print(f"ğŸ” ä»£ç†çŠ¶æ€: HTTP={'http_proxy' in os.environ}, HTTPS={'https_proxy' in os.environ}")
            for proxy_type in ['http_proxy', 'https_proxy']:
                proxy_value = os.environ.get(proxy_type, '')
                proxy_value and print(f"  {proxy_type.upper()}ä»£ç†å€¼: {proxy_value}")
            
            # è°ƒè¯•è¾“å‡ºæ¶ˆæ¯å†å²é•¿åº¦
            messages = initial_state.get("messages", [])
            print(f"ğŸ” å¤„ç†çŠ¶æ€ä¸­åŒ…å«{len(messages)}æ¡æ¶ˆæ¯å†å²")
            
            # æ‰“å°å†å²æ¶ˆæ¯ç”¨äºè°ƒè¯•
            if len(messages) > 1:
                print(f"ğŸ“œ å†å²ä¸Šä¸‹æ–‡æ¦‚è¦:")
                for i, msg in enumerate(messages):
                    role = msg.get("role", "unknown")
                    content = msg.get("content", "")[:30]
                    print(f"  [{i+1}] {role}: {content}...")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœç´¢å…³é”®è¯
            last_query = initial_state.get("query", [""])[0]
            is_search_query = any(kw in last_query for kw in ["æœç´¢", "æŸ¥æ‰¾", "å¯»æ‰¾", "äº¬ä¸œ", "è´­ä¹°", "å•†å“"])
            is_search_query and print(f"ğŸ” æ£€æµ‹åˆ°æœç´¢æŸ¥è¯¢: {last_query}")
            
            # ç”Ÿæˆä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦é¿å…é‡å¤å¤„ç†
            fingerprint = hash(str(time.time()) + str(initial_state.get("query", [])))
            
            # é‡å¤å¤„ç†æ£€æŸ¥
            if fingerprint == self.context.get("history_hash"):
                print("âš ï¸ æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œå¿½ç•¥")
                return
                
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.context["current_turn"] = self.context.get("current_turn", 0) + 1
            self.context["history_hash"] = fingerprint
            self.context["response_cache"] = set()  # é‡ç½®å“åº”ç¼“å­˜
            
            turn = self.context["current_turn"]
            print(f"ğŸ”„ å¼€å§‹å¤„ç†ç¬¬{turn}è½®å¯¹è¯...")
            
            # ç”¨äºå­˜å‚¨å½“å‰è½®æ¬¡çš„æ‰€æœ‰æ¶ˆæ¯ç‰‡æ®µ
            current_response_parts = []
            tool_called, tool_call_start_time = False, None
            step_count = 0
            
            print("ğŸš€ å‡†å¤‡è°ƒç”¨state_graph.astreamå¤„ç†æµç¨‹...")
            
            # ç›´æ¥ä½¿ç”¨æµå¼è°ƒç”¨è·å–å›å¤
            try:
                async for step in self.state_graph.astream(initial_state, {"recursion_limit": 100}):
                    step_count += 1
                    print(f"\nğŸ“ æ‰§è¡Œæ­¥éª¤ {step_count} ...")
                    
                    # è°ƒè¯•æ­¥éª¤å†…å®¹
                    self._debug_step_data(step)
                    
                    # å¤„ç†å·¥å…·è°ƒç”¨æƒ…å†µ
                    if "tool_calls" in step:
                        tool_called = True
                        tool_call_start_time = time.time()
                        tool_info = step.get("tool_calls", [{}])[0]
                        tool_name = tool_info.get("name", "æœªçŸ¥å·¥å…·")
                        print(f"ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_name}")
                        yield f"æ­£åœ¨æ‰§è¡Œæ“ä½œ: {tool_name}..."
                        continue
                    
                    # å·¥å…·è°ƒç”¨è¶…æ—¶æ£€æŸ¥
                    if tool_called and tool_call_start_time:
                        elapsed = time.time() - tool_call_start_time
                        if elapsed > 60:  # å·¥å…·è°ƒç”¨60ç§’è¶…æ—¶
                            print(f"âš ï¸ å·¥å…·è°ƒç”¨è¶…æ—¶! å·²ç»è€—æ—¶ {elapsed:.1f} ç§’")
                            yield "æ“ä½œè¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–äº¬ä¸œæ¥å£æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
                            break
                    
                    # è·å–æ¶ˆæ¯å†…å®¹
                    step_messages = MessageProcessor.parse(step)
                    step_messages and print(f"  è§£æå¾—åˆ° {len(step_messages)} æ¡æ¶ˆæ¯")
                    
                    # ä»å½“å‰æ­¥éª¤ä¸­æå–æœ€æ–°çš„åŠ©æ‰‹æ¶ˆæ¯
                    latest_assistant_msgs = self._extract_latest_assistant_message(step)
                    
                    # å¤„ç†æ–°åŠ©æ‰‹æ¶ˆæ¯
                    if latest_assistant_msgs:
                        print(f"  âœ“ æ‰¾åˆ° {len(latest_assistant_msgs)} æ¡æ–°åŠ©æ‰‹æ¶ˆæ¯")
                        for msg in latest_assistant_msgs:
                            if msg and len(msg.strip()) >= 2:  # è·³è¿‡å¤ªçŸ­çš„æ¶ˆæ¯
                                current_response_parts.append(msg.strip())
                                print(f"  ğŸ“¤ è¾“å‡ºæ¶ˆæ¯: {msg.strip()[:50]}...")
                                yield msg.strip()
                    else:
                        print("  âŒ æœªæ‰¾åˆ°æ–°çš„åŠ©æ‰‹æ¶ˆæ¯")
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»“æŸ
                    if step.get("finished", False):
                        print("âœ“ å¯¹è¯å·²å®Œæˆ")
                        break
                    
                print(f"âœ… æµå¤„ç†å®Œæˆï¼Œå…±æ‰§è¡Œäº† {step_count} ä¸ªæ­¥éª¤")
                
            except Exception as stream_error:
                print(f"âŒ æµå¤„ç†å¼‚å¸¸: {str(stream_error)}")
                print(f"å¼‚å¸¸ç±»å‹: {type(stream_error).__name__}")
                import traceback
                traceback.print_exc()
                yield f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(stream_error)[:100]}"
                    
            # å¤„ç†å“åº”æ±‡æ€»è¾“å‡º
            if current_response_parts:
                combined = " ".join(current_response_parts)
                print(f"âœ“ æœ¬è½®å›å¤æ±‡æ€»: {combined[:100]}...")
            else:
                print("âš ï¸ æœªæ”¶é›†åˆ°ä»»ä½•å“åº”ç‰‡æ®µ")
                yield "æŠ±æ­‰ï¼Œæˆ‘æœªèƒ½ç”Ÿæˆå›å¤ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
                    
        except Exception as e:
            print(f"âŒ å¤„ç†å™¨å¼‚å¸¸: {str(e)}")
            traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            yield f"å¾ˆæŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜ã€‚({str(e)[:50]})"
            
        finally:
            print(f"âœ“ ç¬¬{self.context.get('current_turn', 0)}è½®å¯¹è¯å¤„ç†å®Œæˆ")
            
    def _debug_step_data(self, step):
        """è°ƒè¯•æ­¥éª¤æ•°æ®çš„è¾…åŠ©æ–¹æ³•"""
        try:
            step_keys = list(step.keys()) if isinstance(step, dict) else "éå­—å…¸ç±»å‹"
            print(f"  æ­¥éª¤æ•°æ®ç±»å‹: {type(step).__name__}, åŒ…å«é”®: {step_keys}")
            
            # å¤„ç†æ¶ˆæ¯æ•°æ®
            if isinstance(step, dict) and "messages" in step:
                msgs = step["messages"]
                print(f"  æ¶ˆæ¯åˆ—è¡¨é•¿åº¦: {len(msgs)}")
                if msgs:
                    last_msg = msgs[-1]
                    msg_role = last_msg.get("role", "æœªçŸ¥") if isinstance(last_msg, dict) else type(last_msg).__name__
                    msg_content = str(last_msg.get("content", ""))[:50] if isinstance(last_msg, dict) else str(last_msg)[:50]
                    print(f"  æœ€æ–°æ¶ˆæ¯: {msg_role} -> {msg_content}...")
            
            # æ£€æŸ¥èŠ‚ç‚¹ä¿¡æ¯
            isinstance(step, dict) and "node_name" in step and print(f"  å½“å‰èŠ‚ç‚¹: {step['node_name']}")
            
            # æ£€æŸ¥å®ŒæˆçŠ¶æ€
            isinstance(step, dict) and "finished" in step and print(f"  å®ŒæˆçŠ¶æ€: {step['finished']}")
            
        except Exception as detail_e:
            print(f"  âš ï¸ è§£ææ­¥éª¤è¯¦æƒ…å¤±è´¥: {str(detail_e)}")

    def _extract_latest_assistant_message(self, step_data: Dict) -> List[str]:
        """ä»æ­¥éª¤æ•°æ®ä¸­æå–æœ€æ–°çš„åŠ©æ‰‹æ¶ˆæ¯ï¼Œä¼˜åŒ–å·¥å…·è°ƒç”¨å¤„ç†"""
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tool_calls" in step_data:
            tool_calls = step_data.get("tool_calls", [])
            if not tool_calls:
                return []
                
            # æå–å·¥å…·ä¿¡æ¯å¹¶è¿”å›æ¶ˆæ¯
            try:
                tool_info = tool_calls[0] if isinstance(tool_calls, list) else tool_calls
                tool_name = tool_info.get("name", "æœªçŸ¥å·¥å…·")
                tool_args = json.dumps(tool_info.get("args", {}), ensure_ascii=False)
                
                # æ„å»ºå·¥å…·è°ƒç”¨æ¶ˆæ¯
                tool_msg = f"æ­£åœ¨ä½¿ç”¨{tool_name}å·¥å…·ï¼Œå‚æ•°: {tool_args[:50]}..."
                
                msg_hash = hash(tool_msg)
                if msg_hash not in self.context.get("response_cache", set()):
                    self.context.setdefault("response_cache", set()).add(msg_hash)
                    return [tool_msg]
            except Exception as e:
                print(f"âš ï¸ è§£æå·¥å…·è°ƒç”¨å‡ºé”™: {str(e)}")
            
            return []
            
        # å¤„ç†å·¥å…·å“åº”    
        if "tool_response" in step_data:
            try:
                response = step_data.get("tool_response", {})
                content = response.get("content", "")
                
                if content:
                    msg_hash = hash(content)
                    if msg_hash not in self.context.get("response_cache", set()):
                        self.context.setdefault("response_cache", set()).add(msg_hash)
                        return [f"å·¥å…·æ‰§è¡Œç»“æœ: {str(content)[:200]}..."]
            except Exception as e:
                print(f"âš ï¸ è§£æå·¥å…·å“åº”å‡ºé”™: {str(e)}")
            
            return []
        
        # æ£€æŸ¥å®ŒæˆçŠ¶æ€
        step_data.get("finished", False) and print("âœ“ æ£€æµ‹åˆ°å¯¹è¯å®Œæˆæ ‡å¿—")
        
        # å¤„ç†æ™®é€šæ¶ˆæ¯
        parsed_messages = MessageProcessor.parse(step_data)
        
        if not parsed_messages:
            return []
            
        # è·å–æœ€æ–°çš„åŠ©æ‰‹æ¶ˆæ¯
        latest_message = MessageProcessor.get_latest_assistant_message(parsed_messages)
        if not latest_message:
            return []
            
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
        msg_hash = hash(latest_message)
        if msg_hash in self.context.get("response_cache", set()):
            return []
            
        # æ·»åŠ åˆ°ç¼“å­˜å¹¶è¿”å›
        self.context.setdefault("response_cache", set()).add(msg_hash)
        return [latest_message]

    async def _process_with_timeout(self, task, timeout_seconds):
        """å¸¦è¶…æ—¶æ§åˆ¶çš„å¼‚æ­¥è¿­ä»£å™¨åŒ…è£…å™¨"""
        accumulated_chunks = []
        start_time = time.time()
        print(f"â±ï¸ å¯åŠ¨è¶…æ—¶ä¿æŠ¤ï¼Œæœ€å¤§å¤„ç†æ—¶é—´: {timeout_seconds}ç§’")
        
        try:
            # å¤„ç†ä»»åŠ¡æµ
            chunk_count = 0
            
            async for chunk in task:
                chunk_count += 1
                current_time = time.time()
                elapsed = current_time - start_time
                
                # è®°å½•ç¬¬ä¸€ä¸ªå“åº”æ—¶é—´
                chunk_count == 1 and print(f"â±ï¸ æ”¶åˆ°ç¬¬ä¸€ä¸ªå“åº”ï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
                
                # è¶…æ—¶æ£€æŸ¥
                if elapsed > timeout_seconds:
                    print(f"âš ï¸ å¤„ç†è¶…æ—¶! å·²ç»è€—æ—¶ {elapsed:.1f} ç§’")
                    raise asyncio.TimeoutError(f"å¤„ç†æ—¶é—´è¶…è¿‡{timeout_seconds}ç§’")
                
                # æ”¶é›†å¹¶è¿”å›å—
                accumulated_chunks.append(chunk)
                yield chunk
                
            # è®°å½•å®Œæˆæƒ…å†µ
            completion_time = time.time() - start_time
            print(f"âœ… å¤„ç†æˆåŠŸå®Œæˆï¼Œæ€»è€—æ—¶: {completion_time:.2f}ç§’ï¼Œå¤„ç†äº†{len(accumulated_chunks)}ä¸ªå“åº”ç‰‡æ®µ")
                
        except asyncio.TimeoutError:
            elapsed = time.time() - start_time
            print(f"âš ï¸ å¤„ç†è¶…æ—¶! å·²ç»è€—æ—¶ {elapsed:.1f} ç§’ï¼Œå·²æ”¶é›† {len(accumulated_chunks)} ä¸ªå›å¤ç‰‡æ®µ")
            
            # è¿”å›éƒ¨åˆ†ç»“æœ
            accumulated_chunks and (yield "ç”±äºæ“ä½œæ—¶é—´è¿‡é•¿ï¼Œå¤„ç†è¢«ä¸­æ–­ã€‚ä»¥ä¸‹æ˜¯å·²è·å–çš„éƒ¨åˆ†ä¿¡æ¯ï¼š")
            raise
            
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}")
            print(f"  å¤„ç†å·²è¿›è¡Œ {elapsed:.1f} ç§’ï¼Œæ”¶é›†äº† {len(accumulated_chunks)} ä¸ªç‰‡æ®µ")
            import traceback
            traceback.print_exc()
            
            # è¿”å›éƒ¨åˆ†ç»“æœ
            accumulated_chunks and (yield "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œä»¥ä¸‹æ˜¯éƒ¨åˆ†ç»“æœï¼š")
            raise

# ================== ç¨³å¥ç•Œé¢ç³»ç»Ÿ ==================
class GradioUI:
    def __init__(self, state_graph: StateGraph):
        self.state_graph = state_graph
        self.processor = StreamProcessor(state_graph)
        self.interface = self._build_interface()
        
    # æ·»åŠ ä»£ç†ç®¡ç†ç›¸å…³æ–¹æ³•
    def _enable_proxy(self):
        """å¯ç”¨HTTPå’ŒHTTPSä»£ç†"""
        import os
        # ä¸ºLLMè°ƒç”¨è®¾ç½®ä»£ç†
        os.environ["http_proxy"] = "http://127.0.0.1:7890"
        os.environ["https_proxy"] = "http://127.0.0.1:7890"
        print("âœ… å·²å¯ç”¨ç½‘ç»œä»£ç†")

    def _disable_proxy(self):
        """ç¦ç”¨HTTPå’ŒHTTPSä»£ç†"""
        import os
        # æš‚æ—¶æ¸…é™¤ä»£ç†è®¾ç½®
        if "http_proxy" in os.environ:
            del os.environ["http_proxy"]
        if "https_proxy" in os.environ:
            del os.environ["https_proxy"]
        print("âŒ å·²ç¦ç”¨ç½‘ç»œä»£ç†ï¼Œæœ¬åœ°æœåŠ¡å¯æ­£å¸¸å¯åŠ¨")

    def _build_interface(self):
        """æ„å»ºæŠ—å¡é¡¿ç•Œé¢"""
        import gradio as gr
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»£æ›¿ç›´æ¥é…ç½®
        import os
        os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
        
        with gr.Blocks(title="äº¬ä¸œå¯¼è´­åŠ©æ‰‹", theme=gr.themes.Soft(
            primary_hue="emerald",
            secondary_hue="gray"
        ), css="""
        body {
            background-color: #f7f7f7;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            padding: 15px;
            box-sizing: border-box;
        }
        .gradio-container {
            width: 85vw !important;
            max-width: 1400px !important;
            min-width: 800px !important;
            margin: 0 auto !important;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        .main-div {
            display: flex;
            flex-direction: column;
            width: 100%;
            padding: 15px;
        }
        .loading {color: #888 !important; font-style: italic}
        .message-wrap {width: 95% !important;}
        .message-wrap .bubble {max-width: 85% !important;}
        .chatbot {
            height: 60vh !important; 
            min-height: 400px !important;
            overflow-y: auto;
        }
        .input-row {border-radius: 8px; overflow: hidden; margin-top: 8px;}
        .button-column {display: flex; flex-direction: column; gap: 8px;}
        
        /* å¤´éƒ¨å¸ƒå±€ */
        .header-area {margin-bottom: 10px !important;}
        h1 {margin: 0 !important; padding: 10px 0 5px 0 !important; font-size: 24px !important;}
        p {margin: 0 0 5px 0 !important;}
        
        /* åº•éƒ¨åŒºåŸŸ */
        .footer-area {
            margin-top: 10px;
            border-top: 1px solid #eee;
            padding-top: 10px;
        }
        
        /* å¼ºåˆ¶ä½¿ç”¨æŒ‡å—å§‹ç»ˆæ˜¾ç¤º */
        .guide-section {
            margin: 10px 0 !important;
            border: 1px solid #eaeaea;
            border-radius: 8px;
            background-color: #f8f9fa;
            padding: 8px;
        }
        .guide-section > .hide {display: block !important;}
        .guide-section > .label-wrap {cursor: default !important;}
        .guide-section > .label-wrap > svg {display: none !important;}
        
        /* åº•éƒ¨çŠ¶æ€åŒºåŸŸ */
        .footer {
            text-align: center;
            margin-top: 5px;
            font-size: 0.8em;
            color: #666;
        }
        
        /* å“åº”å¼è°ƒæ•´ */
        @media screen and (max-height: 700px) {
            .chatbot {height: 50vh !important;}
            h1 {font-size: 20px !important;}
        }
        @media screen and (max-width: 1000px) {
            .gradio-container {width: 95vw !important;}
        }
        """, analytics_enabled=False) as interface:
            # é¡µå¤´ - æ·»åŠ ç±»åä¾¿äºæ ·å¼æ§åˆ¶
            with gr.Column(elem_classes="header-area"):
                gr.Markdown("""<div style="text-align: center;">
                    <h1>ğŸ›ï¸ äº¬ä¸œæ™ºèƒ½å¯¼è´­</h1>
                    <p>æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ç§äººè´­ç‰©åŠ©æ‰‹ï¼Œéšæ—¶ä¸ºæ‚¨æ¨èä¼˜è´¨å•†å“</p>
                </div>""")
            
            # èŠå¤©åŒº
            with gr.Column(scale=1, min_width=800):
                chatbot = gr.Chatbot(
                    label="",
                    avatar_images=(
                        "https://cdn-icons-png.flaticon.com/512/1077/1077114.png",  # ç”¨æˆ·å¤´åƒ
                        "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"    # åŠ©æ‰‹å¤´åƒ
                    ),
                    show_copy_button=True,
                    type="messages",  # è®¾ç½®æ¶ˆæ¯ç±»å‹ä¸ºmessages
                    render_markdown=True,  # å¯ç”¨Markdownæ¸²æŸ“
                    sanitize_html=False,   # å…è®¸HTMLå†…å®¹ 
                    height=460,            # è°ƒæ•´é«˜åº¦ä»¥é€‚åº”å›ºå®šå®¹å™¨
                    elem_classes="chatbot"
                )

                # è¾“å…¥åŒº
                with gr.Row(variant="panel", elem_classes="input-row"):
                    input_box = gr.Textbox(
                        placeholder="è¯·è¾“å…¥é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæ¨èæ‰‹æœº...",
                        container=False,
                        scale=7,
                        autofocus=True,
                        max_lines=3,
                        elem_classes="input-box"
                    )
                    with gr.Column(scale=1, elem_classes="button-column"):
                        submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", size="sm")
                        clear_btn = gr.Button("ğŸ§¹ æ¸…ç©º", variant="secondary", size="sm")
            
            # åº•éƒ¨åŒºåŸŸ
            with gr.Column(elem_classes="footer-area"):
                # æ·»åŠ ä½¿ç”¨æŒ‡å—åˆ°åº•éƒ¨
                with gr.Accordion("ä½¿ç”¨æŒ‡å—", open=True, elem_classes="guide-section"):
                    gr.Markdown("""
                    ### ğŸ›ï¸ äº¬ä¸œå¯¼è´­åŠ©æ‰‹ä½¿ç”¨æŒ‡å—
                    
                    1. **æœç´¢å•†å“**: è¾“å…¥"æœç´¢ [å•†å“åç§°]"å³å¯æŸ¥æ‰¾å•†å“
                    2. **æ¯”è¾ƒå•†å“**: å¯ä»¥è¯¢é—®ä¸åŒå•†å“çš„åŒºåˆ«å’Œä¼˜ç¼ºç‚¹
                    3. **è·å–å»ºè®®**: æè¿°æ‚¨çš„éœ€æ±‚ï¼ŒAIä¼šç»™å‡ºé€‚åˆçš„æ¨è
                    4. **æŸ¥çœ‹è¯¦æƒ…**: ç‚¹å‡»å•†å“é“¾æ¥å¯ä»¥ç›´æ¥è·³è½¬åˆ°äº¬ä¸œè´­ä¹°
                    
                    > æç¤ºï¼šæœç´¢ç»“æœä¼šä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºï¼ŒåŒ…å«äº§å“å›¾ç‰‡å’Œè´­ä¹°é“¾æ¥
                    """)
                
                # æ·»åŠ çŠ¶æ€ä¿¡æ¯
                with gr.Row(elem_classes="footer"):
                    gr.Markdown("""<div style="text-align: center; margin-top: 5px; font-size: 0.8em; color: #666; width: 100%;">
                        ç³»ç»ŸçŠ¶æ€: å·²è¿æ¥ | ç‚¹å‡»å‘é€æŒ‰é’®å¼€å§‹å¯¹è¯
                    </div>""")
            
            # äº¤äº’é€»è¾‘
            submit_btn.click(
                self._user_input_handler,
                [input_box, chatbot],
                [input_box, chatbot]
            ).then(
                self._bot_response_handler,
                chatbot,
                chatbot
            )
            
            input_box.submit(
                self._user_input_handler,
                [input_box, chatbot],
                [input_box, chatbot]
            ).then(
                self._bot_response_handler,
                chatbot,
                chatbot
            )
            
            clear_btn.click(
                lambda: [],  # è¿”å›ç©ºåˆ—è¡¨æ¸…é™¤å†å²
                None, 
                chatbot,
                queue=False
            )
                
        # è®¾ç½®é˜Ÿåˆ—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        self._setup_queue(interface)
                
        return interface
        
    def _setup_queue(self, interface):
        """è®¾ç½®é˜Ÿåˆ—å…¼å®¹ä¸åŒç‰ˆæœ¬"""
        queue_config = {"concurrency_count": 5, "max_size": 20}
        
        try:
            # ç›´æ¥å°è¯•è®¾ç½®é˜Ÿåˆ—
            interface.queue(**queue_config)
            return True
        except Exception as e:
            print(f"æ— æ³•ä½¿ç”¨æ–°ç‰ˆé˜Ÿåˆ—API: {str(e)}")
            
            # å°è¯•å…¼å®¹æ¨¡å¼
            try:
                interface.queue()  # ä¸å¸¦å‚æ•°çš„è°ƒç”¨
                print("ä½¿ç”¨å…¼å®¹æ¨¡å¼è®¾ç½®é˜Ÿåˆ—")
                return True
            except:
                print("è­¦å‘Š: æ— æ³•è®¾ç½®é˜Ÿåˆ—ï¼ŒUIå“åº”å¯èƒ½è¾ƒæ…¢")
                return False

    def _user_input_handler(self, user_input: str, history: list):
        """ç”¨æˆ·è¾“å…¥å¤„ç†ï¼ˆå¸¦æ¶ˆæ¯’ï¼‰"""
        # å½“å¼€å¯æ–°å¯¹è¯æ—¶é‡ç½®å¤„ç†å™¨
        len(history) == 0 and self.processor.reset_context()
    
        sanitized_input = user_input.strip()[:500]
        print(f"\nğŸ‘¤ ç”¨æˆ·è¾“å…¥ï¼ˆ{len(sanitized_input)}å­—ï¼‰ï¼š{sanitized_input[:50]}...")
        
        # æ„å»ºå…¼å®¹ messages ç±»å‹çš„æ¶ˆæ¯æ ¼å¼
        new_message = {"role": "user", "content": sanitized_input}
        
        # è¿”å›ç©ºæ–‡æœ¬æ¡†å’Œæ›´æ–°çš„å†å²è®°å½•
        return "", history + [new_message]

    async def _bot_response_handler(self, history: list):
        """ç®€åŒ–çš„å“åº”å¤„ç†å™¨ï¼Œå…¼å®¹messagesç±»å‹"""
        if not history:
            print("âŒ é”™è¯¯: ç©ºå†å²è®°å½•")
            yield []
            return
            
        try:
            # ç¡®ä¿å¯ç”¨ä»£ç†ä»¥ä¾¿LLMè°ƒç”¨
            self._enable_proxy()
            print("âœ… å·²ç¡®ä¿ä»£ç†å¯ç”¨")
            
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            last_message = history[-1]
            if last_message["role"] != "user":
                print("âŒ é”™è¯¯: æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯")
                yield history
                return
                
            last_user_message = last_message["content"]
            print(f"ç”¨æˆ·é—®é¢˜: {last_user_message}")
            
            # æ£€æŸ¥æ˜¯å¦è§¦å‘äº†æœç´¢å…³é”®è¯
            search_keywords = ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾ä¸€ä¸‹", "å¯»æ‰¾", "æŸ¥è¯¢", "æ‰¾æ‰¾"]
            is_search_query = any(keyword in last_user_message for keyword in search_keywords)
            
            # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²
            full_messages = []
            for msg in history[:-1]:  # ä¸åŒ…æ‹¬æœ€åä¸€æ¡å¾…å¤„ç†çš„æ¶ˆæ¯
                # è½¬æ¢ä¸ºapp.pyä¸­JD_QueryStateæ¥å—çš„æ ¼å¼
                role = msg.get("role", "user")
                content = msg.get("content", "")
                full_messages.append({"role": role, "content": content})
                
            # æ·»åŠ æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            full_messages.append({"role": "user", "content": last_user_message})
            
            print(f"ğŸ“š æ„å»ºäº†å®Œæ•´å†å²ä¸Šä¸‹æ–‡ï¼Œå…±{len(full_messages)}æ¡æ¶ˆæ¯")
            
            # åˆ›å»ºå¸¦æœ‰å®Œæ•´å†å²çš„çŠ¶æ€
            initial_state = JD_QueryState(
                messages=full_messages,
                query=[last_user_message],  # å½“å‰æŸ¥è¯¢ä»ç„¶åªåŒ…å«æœ€æ–°æ¶ˆæ¯
                finished=False
            )
            
            print(f"ğŸ”„ åˆ›å»ºåˆå§‹çŠ¶æ€: {str(initial_state)[:100]}...")
            
            # æ ‡è®°å½“å‰å“åº”ä¸ºå¤„ç†ä¸­
            thinking_content = "æ­£åœ¨æœç´¢å•†å“ï¼Œè¯·ç¨å€™...è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´" if is_search_query else "æ€è€ƒä¸­..."
            history.append({"role": "assistant", "content": thinking_content})
            yield history
            
            print("âŒ› æ˜¾ç¤ºæ€è€ƒä¸­çŠ¶æ€ï¼Œå¼€å§‹å¤„ç†...")
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_response = ""
            response_success = False
            
            # å¸¦è¶…æ—¶çš„æµå¼å¤„ç†
            timeout = 120  # ç»Ÿä¸€è¶…æ—¶æ—¶é—´
            print(f"ğŸ“¡ å‡†å¤‡è°ƒç”¨å¤„ç†å™¨...è®¾ç½®è¶…æ—¶æ—¶é—´: {timeout}ç§’")
            
            try:
                process_task = self.processor.process(initial_state)
                response_chunk_count = 0
                
                async for chunk in self.processor._process_with_timeout(process_task, timeout):
                    response_chunk_count += 1
                    
                    # å¤„ç†æœ‰æ•ˆç‰‡æ®µ
                    chunk_text = chunk.strip()
                    if chunk_text:
                        # æ·»åŠ åˆ°å®Œæ•´å“åº”
                        full_response += chunk_text + " "
                        # æ›´æ–°å†å²å¹¶æ˜¾ç¤º
                        history[-1] = {"role": "assistant", "content": full_response.strip()}
                        print(f"ğŸ“¤ æ¥æ”¶åˆ°ç¬¬{response_chunk_count}ä¸ªå“åº”ç‰‡æ®µ: {chunk_text[:30]}...")
                        yield history
                        # è½»å¾®å»¶è¿Ÿä½¿ç•Œé¢æ›´æµç•…
                        await asyncio.sleep(0.05)
                
                print(f"âœ… æµå¤„ç†å®Œæˆï¼Œå…±æ¥æ”¶{response_chunk_count}ä¸ªå“åº”ç‰‡æ®µ")
                response_success = True
                
            except (asyncio.TimeoutError, Exception) as process_error:
                # é”™è¯¯ç±»å‹æ ‡è¯†
                error_type = "è¶…æ—¶" if isinstance(process_error, asyncio.TimeoutError) else "å¼‚å¸¸"
                print(f"âŒ å¤„ç†{error_type}: {str(process_error)}")
                
                # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                isinstance(process_error, asyncio.TimeoutError) or traceback.print_exc()
                
                # æ„å»ºé”™è¯¯æ¶ˆæ¯
                error_prefix = "æœç´¢æ“ä½œè¶…æ—¶ã€‚äº¬ä¸œæœç´¢å¯èƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚" if isinstance(process_error, asyncio.TimeoutError) else f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(process_error)[:100]}"
                
                # æ„å»ºæœ€ç»ˆå“åº”
                final_content = full_response.strip() + "\n\n" + error_prefix if full_response else error_prefix
                history[-1] = {"role": "assistant", "content": final_content}
                yield history
                return
                
            # æœ€ç»ˆå“åº”å¤„ç†
            if response_success:
                clean_response = full_response.strip()
                
                # æ ¹æ®å“åº”æƒ…å†µè®¾ç½®ä¸åŒçš„æ¶ˆæ¯
                if not clean_response:
                    fallback_msg = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å®Œæˆå•†å“æœç´¢ã€‚å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–è€…äº¬ä¸œæ¥å£æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åå†è¯•ã€‚" if is_search_query else "æŠ±æ­‰ï¼Œæˆ‘æ²¡èƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ¢ä¸ªé—®é¢˜è¯•è¯•ã€‚"
                    history[-1] = {"role": "assistant", "content": fallback_msg}
                    print("âš ï¸ æœªæ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆå“åº”")
                else:
                    history[-1] = {"role": "assistant", "content": clean_response}
                    print(f"âœ… æˆåŠŸç”Ÿæˆå›å¤ï¼Œé•¿åº¦:{len(clean_response)}")
                
                print(f"å›å¤: {history[-1]['content'][:50]}...")
                yield history
                
        except Exception as e:
            # å¤„ç†æœ€å¤–å±‚å¼‚å¸¸
            print(f"âŒ å¤„ç†å™¨æ•´ä½“å¼‚å¸¸: {str(e)}")
            traceback.print_exc()
            
            # å°è¯•æ·»åŠ é”™è¯¯å“åº”
            error_msg = f"å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºé”™ï¼Œè¯·é‡è¯•ã€‚é”™è¯¯: {str(e)[:100]}"
            history and history.append({"role": "assistant", "content": error_msg})
            
            yield history

    def _init_environment(self):
        """åˆå§‹åŒ–è¿è¡Œç¯å¢ƒï¼Œå¤„ç†ä»£ç†å†²çª"""
        import os
        
        # è®¾ç½®NO_PROXYç¯å¢ƒå˜é‡ä»¥é¿å…ä»£ç†å†²çª
        local_addresses = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
        os.environ["NO_PROXY"] = ",".join(local_addresses)
        os.environ["no_proxy"] = os.environ["NO_PROXY"]
        print(f"ğŸ”’ å·²è®¾ç½®NO_PROXY={os.environ['NO_PROXY']}ï¼Œç¡®ä¿æœ¬åœ°è¿æ¥ä¸ç»è¿‡ä»£ç†")
        
        # åˆ é™¤å¯èƒ½å¹²æ‰°çš„ä»£ç†è®¾ç½®
        for proxy_var in ["HTTP_PROXY", "HTTPS_PROXY"]:
            proxy_var in os.environ and os.environ.pop(proxy_var)
        
        # è®¾ç½®Gradioç¯å¢ƒå˜é‡
        gradio_env_settings = {
            "GRADIO_ANALYTICS_ENABLED": "False",  # ç¦ç”¨åˆ†æ
            "GRADIO_ALLOW_FLAGGING": "never",     # ç¦ç”¨æ ‡è®°
            "GRADIO_THEME": "soft",               # ä½¿ç”¨è½¯ä¸»é¢˜
            "GRADIO_USE_PROXY": "false"           # ç¦ç”¨Gradioä»£ç†
        }
        
        # æ‰¹é‡åº”ç”¨ç¯å¢ƒå˜é‡
        for key, value in gradio_env_settings.items():
            os.environ[key] = value
        
        # è¿”å›ä»£ç†çŠ¶æ€
        return "http_proxy" in os.environ or "https_proxy" in os.environ

    def launch(self, **kwargs):
        """å¯åŠ¨ç¨³å¥æœåŠ¡"""
        # åˆå§‹åŒ–ç¯å¢ƒ
        self._init_environment()
        
        # è®¾ç½®å¯åŠ¨å‚æ•°ï¼Œåˆå¹¶é»˜è®¤å‚æ•°å’Œä¼ å…¥å‚æ•°
        launch_kwargs = {
            # é»˜è®¤å‚æ•°
            "server_name": "0.0.0.0",  # ä½¿ç”¨0.0.0.0å…è®¸ä»ä»»ä½•IPè®¿é—®
            "server_port": 7861,       # ä½¿ç”¨å›ºå®šç«¯å£
            "share": False,            # é»˜è®¤ä¸åˆ†äº«
            "inbrowser": True,         # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
            "max_threads": 30,         # æ›´å¤šçº¿ç¨‹
            "quiet": False,            # æ˜¾ç¤ºæ‰€æœ‰æ—¥å¿—
            **kwargs                   # åˆå¹¶ä¼ å…¥çš„å‚æ•°
        }
        
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨UIæœåŠ¡ï¼Œå‚æ•°: {launch_kwargs}")
        
        # ä¸´æ—¶ç¦ç”¨ä»£ç†ä»¥å¯åŠ¨æœåŠ¡å™¨
        self._disable_proxy()
        
        # ç¡®ä¿å¯åŠ¨åæ¢å¤ä»£ç†
        result = None
        try:
            result = self.interface.launch(**launch_kwargs)
        except Exception as e:
            print(f"âŒ å¯åŠ¨å‡ºé”™: {str(e)}")
            raise
        finally:
            # æ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½æ¢å¤ä»£ç†è®¾ç½®
            self._enable_proxy()
            
        return result

    @staticmethod
    def is_search_query(query: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœç´¢æŸ¥è¯¢"""
        search_keywords = ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾ä¸€ä¸‹", "å¯»æ‰¾", "æŸ¥è¯¢", "æ‰¾æ‰¾", "äº¬ä¸œ", "è´­ä¹°", "å•†å“"]
        product_keywords = ["æ‰‹æœº", "ç”µè„‘", "ç¬”è®°æœ¬", "ROG", "åä¸º", "è‹¹æœ", "å°ç±³", "ç”µè§†", "è€³æœº"]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœç´¢å…³é”®è¯
        has_search_kw = any(kw in query for kw in search_keywords)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº§å“å…³é”®è¯
        has_product_kw = any(kw in query for kw in product_keywords)
        
        # åŒæ—¶åŒ…å«æœç´¢å…³é”®è¯å’Œäº§å“å…³é”®è¯çš„å¯èƒ½æ˜¯æœç´¢è¯·æ±‚
        return has_search_kw and has_product_kw

if __name__ == "__main__":
    # å¯¼å…¥å¿…è¦æ¨¡å—
    import os, sys, traceback
    
    try:
        # åˆ›å»ºUIå®ä¾‹
        from app import graph_with_tools
        ui = GradioUI(graph_with_tools)
        
        # é…ç½®ç¯å¢ƒ
        # è®¾ç½®NO_PROXYç¯å¢ƒå˜é‡
        local_addresses = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
        os.environ["NO_PROXY"] = os.environ["no_proxy"] = ",".join(local_addresses)
        print(f"ğŸ”’ å·²è®¾ç½®NO_PROXY={os.environ['NO_PROXY']}ï¼Œç¡®ä¿æœ¬åœ°è¿æ¥ä¸ç»è¿‡ä»£ç†")
        
        # ç¯å¢ƒå‡†å¤‡
        ui._disable_proxy()
        print("âœ… ä¸´æ—¶å–æ¶ˆä»£ç†è®¾ç½®ï¼Œé˜²æ­¢æœ¬åœ°è®¿é—®å†²çª")
        
        # è®¾ç½®Gradioç¯å¢ƒå˜é‡
        os.environ.update({
            "GRADIO_ANALYTICS_ENABLED": "False",
            "GRADIO_USE_PROXY": "false"
        })
        
        # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
        print(f"âœ“ Gradioç‰ˆæœ¬: {gr.__version__}")
        print("âœ… æˆåŠŸå¯¼å…¥å›¾å¯¹è±¡")
        
        # å¯åŠ¨UI
        ui.launch()
        
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {str(e)}")
        print("\n--- è¯¦ç»†é”™è¯¯ä¿¡æ¯ ---")
        traceback.print_exc()
        print("""
        ===== è§£å†³å»ºè®® =====
        1. æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt
        2. éªŒè¯gradioå®‰è£…: pip install --upgrade gradio
        3. æ£€æŸ¥é˜²ç«å¢™æ˜¯å¦å…è®¸æœ¬åœ°ç«¯å£è®¿é—®
        4. å°è¯•å…¶ä»–ç«¯å£: export GRADIO_SERVER_PORT=8000
        5. å¦‚æœä½¿ç”¨Clashï¼Œå°è¯•åœ¨Clashè®¾ç½®ä¸­æ·»åŠ "localhost,127.0.0.1"åˆ°ç»•è¿‡ä»£ç†åˆ—è¡¨
        """)
    finally:
        # ç¡®ä¿æ¢å¤ä»£ç†è®¾ç½®
        try:
            ui._enable_proxy()
            print("âœ“ ä»£ç†è®¾ç½®å·²æ¢å¤")
        except:
            pass
